@InProceedings{Orabona16,
	author = {Orabona, Francesco and P\'al, D\'avid},
	title = {Open Problem: Parameter-Free and Scale-Free Online Algorithms},
	pages = {},
	abstract = {
		Existing vanilla algorithms for online linear optimization have $O((\eta R(u) +
		1/\eta) \sqrt{T})$ regret with respect to any competitor $u$, where $R(u)$ is a
		$1$-strongly convex regularizer and $\eta > 0$ is a tuning parameter of the
		algorithm. For certain decision sets and regularizers, the so-called
		\emph{parameter-free} algorithms have $\widetilde O(\sqrt{R(u) T})$ regret with
		respect to any competitor $u$.  Vanilla algorithm can achieve the same bound
		only for a fixed competitor $u$ known ahead of time by setting $\eta =
		1/\sqrt{R(u)}$. A drawback of both vanilla and parameter-free algorithms is that
		they assume that the norm of the loss vectors is bounded by a constant known to
		the algorithm. There exist \emph{scale-free} algorithms that have $O((\eta R(u) +
		1/\eta) \sqrt{T} \max_{1 \le t \le T} \norm{\ell_t})$ regret with respect to any
		competitor $u$ and for any sequence of loss vector $\ell_1, \dots, \ell_T$.
		Parameter-free analogue of scale-free algorithms have never been designed. Is is
		possible to design algorithms that are simultaneously \emph{parameter-free} and
		\emph{scale-free}?
	}
}
