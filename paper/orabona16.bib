@InProceedings{Orabona16,
	author = {Orabona, Francesco and P\'al, D\'avid},
	title = {Open Problem: Parameter-Free and Scale-Free Online Algorithms},
	pages = {},
	abstract = {
		Existing vanilla algorithms for online linear optimization have O((eta R(u) +
		1/eta)T) regret with respect to any competitor u, where R(u) is a 1-strongly
		convex regularizer and eta > 0 is a tuning parameter of the algorithm.  For
		certain decision sets and regularizers, the so-called parameter-free
		algorithms have O(sqrt{R(u) T}) regret with respect to any competitor u.
		Vanilla algorithm can achieve the same bound only for a fixed competitor u
		known ahead of time by setting eta = 1/R(u).  A drawback of both vanilla and
		parameter-free algorithms is that they assume that the norm of the loss
		vectors is bounded by a constant known to the algorithm. There exist
		scale-free algorithms that have O((eta R(u) + 1/ eta) T max l_t) regret with
		respect to any competitor u and for any sequence of loss vector l_1, ...,
		l_T. Parameter-free analogue of scale-free algorithms have never been
		designed. Is is possible to design algorithms that are simultaneously
		parameter-free and scale-free?}
}
