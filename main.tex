% \documentclass[anon]{colt2016} % Anonymized submission
\documentclass{colt2016} % Include author names

\usepackage[nolist]{acronym}
\usepackage{algorithm,algorithmic}
\usepackage{times}
\usepackage{enumerate}

\DeclareMathOperator{\Regret}{Regret}
\DeclareMathOperator{\Reward}{Reward}
\DeclareMathOperator{\polylog}{polylog}

\newcommand{\R}{\mathbb{R}}     % real numbers
\renewcommand{\H}{\mathcal{H}}  % Hilbert space
\newcommand{\KL}[2]{D\left({#1}\middle\|{#2}\right)}  % KL divergence
\newcommand{\norm}[1]{\left\|{#1}\right\|}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\newcommand{\indicator}{\mathbf{1}}

\begin{acronym}
\acro{OMD}{Online Mirror Descent}
\acro{OCO}{Online Convex Optimization}
\acro{OLO}{Online Linear Optimization}
\acro{LEA}{Learning with Expert Advice}
\end{acronym}

\coltauthor{%
   \Name{Francesco Orabona} \Email{francesco@orabona.com}\\
   \Name{D\'avid P\'al} \Email{dpal@yahoo-inc.com}\\
{\addr Yahoo Research, New York}
}

\title{Open Problem: Parameter-Free and Scale-Free Online Algorithms}

\begin{document}

\maketitle

\begin{abstract}
Existing vanilla algorithms for online linear optimization have
$O((\eta R(u) + 1/\eta) \sqrt{T})$ regret with respect to competitor $u$,
where $R(u)$ is a $1$-strongly convex regularizer and $\eta > 0$ is a
tuning parameter of the algorithm. For certain decision sets, the
so-called \emph{parameter-free} algorithms have been developed, which have
$\widetilde O(\sqrt{R(u) T})$ regret with respect to any competitor $u$.
Vanilla algorithm can achieve the same bound only for a fixed competitor
$u$ known ahead of time by setting $\eta = 1/\sqrt{R(u)}$. A drawback of
both vanilla and parameter-free algorithms is that they assume that loss vectors
have norms bounded by $1$. There exist \emph{scale-free} algorithms that have
$O((\eta R(u) + 1/\eta) \sqrt{T} \max_{1 \le t \le T} \norm{\ell_t})$
regret with respect to any competitor $u$ and for any sequence of loss
vector $\ell_1, \ell_2, \dots, \ell_T$. Parameter-free analogue of scale-free
algorithms have never been designed. Is is possible to design algorithms that
are both \emph{parameter-free} and \emph{scale-free} at the same time?
\end{abstract}

\section{Introduction}

Online linear optimization (OLO) is a sequential decision making problem where,
in each round $t$, an algorithm chooses a point $w_t$ from a convex
\emph{decision set} $K$ and then receives a loss vector $\ell_t$. Algorithm's
goal is to keep its cumulative loss $\sum_{t=1}^T \langle \ell_t, w_t \rangle$
small. We compare algorithms loss with a loss of hypothetical strategy that in
every round chooses the same point $u$; the difference of algorithm's  loss and
the loss of the hypothetical strategy is called regret. More formally,
\emph{regret with respect to a competitor $u \in K$ after $T$ rounds} is
$$
\Regret_T(u) = \sum_{t=1}^T \langle \ell_t, w_t \rangle - \sum_{t=1}^T \langle \ell_t, u \rangle \; .
$$
For more details see~\cite{Cesa-Bianchi-Lugosi-2006, Shalev-Shwartz-2011}.

Algorithms for various decision sets have been investigated (the probability
simplex, various combinatorial polytopes, Hilbert space, unit ball in Hilbert
space). We focus on two particular sets, the $N$-dimensional probability
simplex $\Delta_N = \{ x \in \R^N ~:~ x \ge 0, \norm{x}_1 = 1\}$ and the
Hilbert space.\footnote{The particular type of Hilbert space does not play any
role. To fix ideas, consider the space of infinite sequences of real numbers
that are square-summable.} OLO over $\Delta_N$ is referred to as the problem of
Learning with Expert Advice (LEA); it used in machine learning as a way of
combining $N$ predictors and in boosting. Premier application of OLO over the
Hilbert space is learning of generalized linear models (e.g. logistic
regression) in high dimension.

Shannon entropy is $H(u) = -\sum_{i=1}^N u_i \ln u_i$ defined for any $u \in
\Delta_N$. For any $p \in [1,\infty]$, $\norm{\cdot}_p$ denotes $p$-norm in
$\R^N$. If $\H$ is a real Hilbert space, we denote by $\langle \cdot, \cdot
\rangle$ its inner product, and by $\norm{\cdot}$ the induced norm.

\section{Learning with Expert Advice}

Hedge algorithm~\citep{Freund-Schapire-1997} for LEA satisfies $\Regret_T(u)
\le \sqrt{T \ln(N)}$ for any $u \in \Delta_N$. This is known to be optimal in
the worst-case sense. However, Hedge assumes that $T$ is known in advance and,
more crucially, $\ell_1, \ell_2, \dots, \ell_T \in [0,1]^N$.

Given $p \ge 0$, one can compute a learning rate $\eta > 0$ for
Hedge algorithm such that for all $u$ such that $H(u) \ge p$, Hedge with
learning rate $\eta$ satisfies $\Regret_T(u) \le \sqrt{T (\ln N - p + 1)}$. The
problem of course is this algorithm needs as an input the threshold
$p$.~\footnote{For $p = 0$, Hedge satisfies  $\Regret_T(u) \le \sqrt{T \ln N}$
bound for all $u$.}

There have been tremendous amount of work \citep{Chaudhuri-Freund-Hsu-2009,
Chernov-Vovk-2010, Koolen-van-Erven-2015, Luo-Schapire-2015} on algorithms that
satisfy $\Regret_T(u) \le \widetilde O(\sqrt{T (\ln N - H(p))})$ \textbf{for all
$u \in \Delta_N$}. Recently, \cite{Orabona-Pal-2016-parameter-free} proposed a
simple algorithm that satisfies $\Regret_T(u) \le \sqrt{3 T(4 + \ln N -
H(u))}$ for all $u \in \Delta_N$. Algorithms of this type are called
\emph{parameter-free} algorithms, since in contrast to Hedge they do not need
to know $p$. Alternatively, they have been called algorithms
for \emph{unknown number of experts}. The reason for the latter name is that in
order to compete with any $\epsilon$-fraction of the experts, on can chooses
competitor $u$ that is uniform over these experts and have zero mass on the
remaining ones; such competitor satisfies $H(u) = \ln (\epsilon N)$ and regret
with respect to any such $u$ is $O(\sqrt{T \ln(1/\epsilon)})$.

In another direction, AdaHedge
algorithm~\citep{de-Rooij-van-Erven-Grunwald-Koolen-2014} lifts the assumptions
that $T$ is known and $\ell_t \in [0,1]^N$. Namely, for any sequence of loss
vectors $\{\ell_t\}_{t=1}^\infty$, any $T \ge 0$ and any $u \in \Delta_N$,
AdaHedge satisfies $\Regret_T(u) \le \sqrt{\ln N \sum_{t=1}^T
\norm{\ell_t}_\infty^2}$. AdaHedge is \emph{scale-free} which means that its
predictions $w_t$ are the same for $\{\ell_t\}_{t=1}^\infty$ and $\{c
\ell_t\}_{t=1}^\infty$ where $c$ is any positive constant.

Similar to Hedge, one can try get a better bound for a subset of $\Delta_N$.
Given $p > 0$, one can compute learning rate such that for all $u$ such that
$H(u) \ge p$, AdaHedge algorithm satisfies $\Regret_T(u) \le \sqrt{(\ln N - p +
1) \sum_{t=1}^T \norm{\ell_t}_\infty^2}$. An algorithm that would achieve
similar bound without the knowledge of $p$ is not known.

Our first open problem is to design an algorithm that for any
sequence of loss vectors $\{\ell_t\}_{t=1}^\infty$, any $T \ge 0$,
and any $u \in \Delta_N$ satisfies
$$
\Regret_T(u) \le C \sqrt{(1 + \ln N - H(u)) \sum_{t=1}^T \norm{\ell_t}_\infty^2} \; ,
$$
where $C > 0$ is a universal constant.

\section{OLO over Hilbert Spaces}

The situation with algorithms for OLO over a Hilbert space $\H$ is very similar
to that of LEA.

It is a simple exercise to show that Follow The Regularized Leader (FTRL)
algorithm with regularizer $\frac{1}{2}\norm{\cdot}^2$ and learning rate
$1/\sqrt{T}$ satisfies\footnote{For constant learning rate, the algorithm is
equivalent to gradient descent. However, gradient descent is the ``wrong'' algorithm
for non-constant learning rates; see~\cite{Orabona-Pal-2016-scale-free}.}
$\Regret_T(u) \le \frac{1}{2}(1 + \norm{u}^2) \sqrt{T}$ for all $u \in \H$
assuming that $T$ is known and that $\norm{\ell_1}, \norm{\ell_2}, \dots,
\norm{\ell_T} \le 1$.

Given $\eta > 0$, FTRL with the same regularizer and
learning rate $\eta/\sqrt{T}$ satisfies $\Regret_T(u) \le \eta \sqrt{T}$
for all $u$ such that $\norm{u} \le \eta$. Thus effectively replacing
$\frac{1}{2}(1 + \norm{u}^2)$ with $\eta \ge \norm{u}$.

There have been a lot of work \citep{Streeter-McMahan-2012, Orabona-2013,
McMahan-Abernethy-2013, McMahan-Orabona-2014} on algorithms that satisfy
$\Regret_T(u) \le \widetilde O(\sqrt{\norm{u} T})$ for all $u \in \H$.
The $\widetilde O$ notation hides poly-logarithmic factors in $\norm{u}$ and
$T$. These are called \emph{parameter-free} algorithms, since they do need to
know $\eta$.

\emph{Scale-free} version of FTRL with the same regularizer but with learning
rate $1/\sqrt{\sum_{i=1}^{t-1} \norm{\ell_i}^2}$ satisfies $\Regret_T(u) \le
(6.25 + \frac{1}{2}\norm{u}^2) \sqrt{T} \max_{1 \le t \le T} \norm{\ell_t}$ for
any $T \ge 0$ and any sequence of loss vectors $\{\ell_t\}_{t=1}^\infty$~\citep{Orabona-Pal-2015}. With learning
rate $\eta/\sqrt{\sum_{i=1}^{t-1} \norm{\ell_i}^2}$, the algorithm satisfies
$\Regret_T(u) \le 3.6 \norm{u} \sqrt{T} \max_{1 \le t \le T} \norm{\ell_t}$ for
all $u \in \H$ such that $\norm{u} \le \eta$. However for that the knowledge
of $\eta$ is required.

Our second open problem is to design an algorithm that for any
sequence of loss vectors $\{\ell_t\}_{t=1}^\infty$, any $T \ge 0$,
and any $u \in \H$ satisfies
$$
\Regret_T(u) \le \polylog(1 + \norm{u}, T) \cdot (1 + \norm{u}) \sqrt{T} \max_{1 \le t \le T} \norm{\ell_t} \; ,
$$
where $\polylog(1 + \norm{u}, T)$ is a function that is bounded by a polynomial
of $\log(1 + \norm{u})$ and $\log T$.

\bibliography{biblio}

\appendix

\section{Follow The Regularized Leader for Hilbert Space}

Let $L_t = \sum_{i=1}^t \ell_i$.
Follow The Regularized Leader with learning rate $\eta' > 0$ and regularizer $\frac{1}{2}\norm{w}^2$
chooses in round $t$,
$$
w_t
= \argmin_{w \in \H} \left( \frac{1}{2}\norm{w}^2 + \eta' \sum_{i=1}^{t-1} \langle \ell_i, w \rangle \right)
= - \eta' \sum_{i=1}^{t-1} \ell_i = - \eta' L_{t-1} \; .
$$

Since $\frac{1}{2}\norm{a-b}^2 \ge 0$ for any $a,b \in \H$, we have $\frac{1}{2}\norm{a}^2 \ge \langle a, b \rangle - \frac{1}{2}\norm{b}^2$.
Plugging $a = -\sqrt{\eta'} L_T$ and $b = u/\sqrt{\eta'}$, we have
$$
\frac{1}{2\eta'} \norm{u}^2 + \frac{\eta'}{2} \sum_{t=1}^T \norm{L_t}^2 - \norm{L_{t-1}}^2
= \frac{1}{2\eta'} \norm{u}^2 + \frac{\eta'}{2} \norm{L_T}^2  \ge - \langle L_T, u \rangle
= - \sum_{t=1}^T \langle \ell_t, u \rangle  \; .
$$
Adding $\sum_{t=1}^T \langle \ell_t, w_t \rangle$ to both sides, we have
$$
\Regret_T(u) \le \frac{1}{2\eta'} \norm{u}^2 + \frac{\eta'}{2} \sum_{t=1}^T \norm{L_t}^2 - \norm{L_{t-1}}^2 + \sum_{t=1}^T \langle \ell_t, w_t \rangle \; .
$$
Substuting $w_t = -\eta' L_{t-1}$ and $\norm{L_{t}}^2 = \norm{L_{t-1} + \ell_t}^2 = \norm{L_{t-1}}^2 + \norm{\ell_t}^2 + 2 \langle \ell_t, L_{t-1} \rangle$,
we have
$$
\Regret_T(u) \le \frac{1}{2\eta'} \norm{u}^2 + \frac{\eta'}{2} \sum_{t=1}^T \norm{\ell_t}^2 \; .
$$
Assuming $\norm{\ell_t} \le 1$ and $\eta' = \eta/\sqrt{T}$, we have
$$
\Regret_T(u) \le \left(\frac{1}{2\eta} \norm{u}^2 + \frac{\eta}{2} \right) \sqrt{T} \; .
$$


\end{document}
